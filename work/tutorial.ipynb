{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cortical Magnification Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Author**: &nbsp;&nbsp; [Noah C. Benson](mailto:nben@nyu.edu)  \n",
    "**Date**: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Jan. 10, 2020  \n",
    "**Link**: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [noahbenson/cortical-magnification-tutorial](https://github.com/noahbenson/cortical-magnification-tutorial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended as an introduction to cortical magnification and how it is calculated. When run in its relevant docker-image, with correctly configured HCP credentials (or with additional data that has been configured correctly), it should be possible to use this notebook to calculate and export cortical magnification values for any subject whose retinotopic maps (pRF parameters) have been solved via a pRF solver such as [analyzePRF](https://kendrickkay.net/analyzePRF/), [Vistasoft](https://github.com/vistalab/vistasoft), or [Popeye](https://github.com/kdesimone/popeye)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is cortical magnification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cortical magnification function (CMF) is the function $f(\\mathbf{u}) = m$ where $\\mathbf{u}$ is the $(x,y)$ position in the visual field (typically in degrees of visual angle), and $m$ is the number of mm$^2$ of cortex per deg$^2$ of the visual field at that point in the visual field. Accordingly, a high cortical magnification, e.g. $m(\\mathbf{u})$ = 150 mm$^2$/deg$^2$, indicates that there is a lot of cortex devoted to a relatively small amount of the visual field, while a low value indicates that very little cortical surface is devoted to that part of the visual field. In the human visual system, the fovea tends to have a cortical magnification that is many times larger than that of the periphery.\n",
    "\n",
    "Occasionally, cortical magnification is also reported in terms of mm/deg--linear cortical magnification. This is the form a measurement would take if, for example, we measured the length (in mm) of the 5° iso-eccentricity contour across the surface of V1 then divided it by the length of the equivalent 5° iso-eccentricity arc in the visual field ($\\pi/2 \\times 5° \\approx 7.85°$). When cortical magnification is in units of mm/deg, it typically is measuring the magnification in a particular direction such as tangential (around the visual field) or radial (toward/from the fovea). In the case of the 5° iso-eccentricity arc, the direction would be tangential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and Configure Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this tutorial, we must have a few things configured. First, we should start by importing a few relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some standard/utility libraries:\n",
    "import os, sys, time, h5py, zipfile\n",
    "import six           # six provides python 2/3 compatibility\n",
    "\n",
    "# Import our numerical/scientific libraries, scipy and numpy:\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# The pimms (Python Immutables) library is a utility library that enables lazy\n",
    "# computation and immutble data structures; https://github.com/noahbenson/pimms\n",
    "import pimms\n",
    "\n",
    "# The neuropythy library is a swiss-army-knife for handling MRI data, especially\n",
    "# anatomical/structural data such as that produced by FreeSurfer or the HCP.\n",
    "# https://github.com/noahbenson/neuropythy\n",
    "import neuropythy as ny\n",
    "\n",
    "# Import graphics libraries:\n",
    "# Matplotlib/Pyplot is our 2D graphing library:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# We also use the 3D graphics library ipyvolume for 3D surface rendering\n",
    "import ipyvolume as ipv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure matplotlib for 2D plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These \"magic commands\" tell matplotlib that we want to plot figures inline and\n",
    "# That we are using qt as a backend; due to bugs in certain versions of\n",
    "# matplotlib, we put them in a separate cell from the import statements above\n",
    "# and the configuration statements below.\n",
    "%gui qt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional matplotlib preferences:\n",
    "font_data = {'family':'sans-serif',\n",
    "             'sans-serif':['Helvetica Neue', 'Helvetica', 'Arial'],\n",
    "             'size': 10,\n",
    "             'weight': 'light'}\n",
    "mpl.rc('font',**font_data)\n",
    "# we want relatively high-res images, especially when saving to disk.\n",
    "mpl.rcParams['figure.dpi'] = 72*2\n",
    "mpl.rcParams['savefig.dpi'] = 72*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configure neuropythy's HCP interface, if not done automatically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you intend to use a custom subject and set of pRF measurements with this notebook instead of using an HCP subject, you can skip this section.\n",
    "\n",
    "The [neuropythy](https://github.com/noahbenson/neuropythy) library can easily be configured to automatically download HCP data it is requested. In order to do this, however, it must have been given a set of HCP credentials. The HCP uses the Amazon S3 so these credentials are in the form of a \"key\" and a \"secret\". To obtain HCP credentials, you must register at the [HCP database website](https://db.humanconnectome.org/) then generate Amazon S3 credentials through their interface. The [neuropythy configuration documentation](https://github.com/noahbenson/neuropythy/wiki/Configuration) explains how to do this in more detail.\n",
    "\n",
    "Your credentials will look something like \"`AKAIG8RT71SWARPYUFUS`\" and \"`TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4`\" (key and secret). They are often printed with a \"`:`\" between them like \"`AKAIG8RT71SWARPYUFUS:TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4`\". If, when you started up the docker-container running this notebook, you provided your credentials as a `:`-separated string like this via the environment variable HCP_CREDENTIALS, you should be fine. If neuropythy found your credentials, they should be in neuropythy's `config` structure, which behaves like a Python dictionary. However, if you are running this tutorial in the Neurohackademy 2019 class itself, you likely were not able to do this and will have to set these credentials manually. If neuropythy could not read your credentials or if they were not set, then `ny.config['hcp_credentials']` will be `None`. If this is the case, you can either:\n",
    " * set the credentials directly in this notebook by running something like:  \n",
    "   ```python\n",
    "   key = 'AKAIG8RT71SWARPYUFUS'\n",
    "   secret = 'TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4'\n",
    "   ny.config['hcp_credentials'] = (key, secret)\n",
    "   ```\n",
    " * restart the docker container after configuring the `HCP_CREDENTIALS` environment variable:\n",
    "   ```bash\n",
    "   > export HCP_CREDENTIALS=\"AKAIG8RT71SWARPYUFUS:TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4\"\n",
    "   > docker-compose up\n",
    "   ```\n",
    " * store the credentials in a local file and import its contents into the `HCP_CREDENTIALS` environment variable:\n",
    "   ```bash\n",
    "   > echo \"AKAIG8RT71SWARPYUFUS:TJ/9SJF+AF3J619FA+FAE83+AF3318SXN/K31JB19J4\" > ~/.hcp-passwd\n",
    "   > export HCP_CREDENTIALS=\"`cat ~/.hcp-passwd`\"\n",
    "   > docker-compose up\n",
    "   ```\n",
    "\n",
    "We also want to check that neuropythy was able to connect to the HCP database. If this fails, it is possible that either your credentials are incorrect/expired or that you do not have a valid internet connection, or that you did something unexpected when mounting volumes into the docker that prevent neuropythy from knowing where to store the HCP data (unlikely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No valid HCP credentials were found!\nSee above instructions for remedy.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-866fd0205de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check that HCP credentials were found:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mny\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hcp_credentials'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     raise Exception('No valid HCP credentials were found!\\n'\n\u001b[0m\u001b[1;32m      4\u001b[0m                     'See above instructions for remedy.')\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No valid HCP credentials were found!\nSee above instructions for remedy."
     ]
    }
   ],
   "source": [
    "# Check that HCP credentials were found:\n",
    "if ny.config['hcp_credentials'] is None:\n",
    "    raise Exception('No valid HCP credentials were found!\\n'\n",
    "                    'See above instructions for remedy.')\n",
    "\n",
    "# Check that we can access the HCP database:\n",
    "# To do this we grab the 's3fs' object from neuropythy's 'hcp' dataset; this\n",
    "# object maintains a connection to Amazon's S3 using the hcp credentials. We use\n",
    "# it to perform a basic ls operation on the S3 filesystem. If this fails, we do\n",
    "# not have a working connection to the S3.\n",
    "try: files = ny.data['hcp'].s3fs.ls('hcp-openaccess')\n",
    "except Exception: files = None\n",
    "if files is None:\n",
    "    raise Exception('Could not communicate with S3!\\n'\n",
    "                    'This probably indicates that your credentials are wrong'\n",
    "                    ' or that you do not have an internet connection.')\n",
    "\n",
    "print('Configuration appears fine!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup a subject and the data we need for cortical magnification. Because we are using an HCP subject, most of this is done for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = 177746\n",
    "sub = ny.hcp_subject(sid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alternately, configure a custom data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you intend to use this notebook with an HCP subject rather than with your own subject, you can skip this section.\n",
    "\n",
    "As an alternative to using HCP data, you can configure this notebook to work with a separate data source. If you are using this notebook via the docker-image or the docker-compose command, you will have to make sure that the data you wish to load is already mounted into the docker-image when it is run. If you are using `docker-compose up` or `docker run` to start this notebook, you'll need to restart the notebook using `docker run` while giving it some additional arguments. You'll want to provide your FreeSurfer and/or HCP subject directory to the docker-image, and you'll want to provide a directory containing any data files to the docker as well.\n",
    "\n",
    "Let's assume that you are using a FreeSurfer subject whose path is `/Volumes/server/Freesurfer_subjects/test-sub` and that the data for this subject is in a set of mgz files in the directory `/Users/nben/Data/retinotopy/test-sub`. You would want to start the docker-image with the following command.\n",
    "\n",
    "```bash\n",
    "> docker run --rm -it \\\n",
    "     -v /Volumes/server/Freesurfer_subjects/test-sub:/subjects/test-sub \\\n",
    "     -v /Users/nben/Data/retinotopy/test-sub:/data/test-sub \\\n",
    "     -v \"${PWD}/work\":/home/jovyan/work \\\n",
    "     -p 8765:8765 \\\n",
    "     nben/neuropythy:latest notebook\n",
    "```\n",
    "\n",
    "This makes the `/Volumes/server/Freesurfer_subjects/test-sub` directory available to this notebook as the `/subjects/test-sub` directory and the `/Users/nben/Data/retinotopy/test-sub` directory available as the `/data/test-sub` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit these variables to be appropriate for the data you mounted\n",
    "\n",
    "# The subject directory and the kind of subject it is\n",
    "subject_path = '/subjects/test-sub'\n",
    "subject_type = 'freesurfer' # or 'hcp'\n",
    "#subject_type = 'hcp' # or 'freesurfer'\n",
    "\n",
    "# The directory containing the pRF parameters\n",
    "prf_path = '/data/test-sub'\n",
    "# how the files are named:\n",
    "prf_filename_patt = '{hemi}.prf_{prop}.mgz'\n",
    "# the prf property names as keys, what are the {prop} taks in the file?\n",
    "prf_props = {'polar_angle':        'angle',\n",
    "             'eccentricity':       'eccen',\n",
    "             'radius':             'prfsz',\n",
    "             'variance_explained': 'vexpl'}\n",
    "# We may also want to import inferred maps; we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we measure cortical magnification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuropythy as ny\n",
    "import numpy as np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
